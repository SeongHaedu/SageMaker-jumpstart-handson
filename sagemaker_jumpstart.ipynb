{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3c82c2-62af-41a2-b4b8-bd85fe25a2d1",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad48a72-a11d-4c13-8b65-b4a46b9f2a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1f640ac-529e-4a80-8929-d4691f4bfb1d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Upgrade some libraries for running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c59ea07-e0f7-4444-ba36-74a53fb2a8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.213.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.34.55)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.22.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.25.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.0)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.2)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.21.1)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.2.0)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.0.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.26.18)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.66.1)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.55 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.34.55)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.1.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from docker->sagemaker) (1.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (2024.2.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.33.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.17.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2023.4)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.8)\n",
      "Requirement already satisfied: dill>=0.3.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.16)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker --upgrade\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e3998-72cc-4798-836b-5217259e07e3",
   "metadata": {},
   "source": [
    "In order to train and host with Amazon Sagemaker, you need to set up and authenticate to use AWS services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d8a7e014-b0d7-4425-90eb-c5d57780fd78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----AWS authentication info------\n",
      "Role: arn:aws:iam::222426255091:role/SageMaker-local-role\n",
      "Region: us-east-1\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "print('-----AWS authentication info------')\n",
    "print('Role: ' + aws_role)\n",
    "print('Region: ' + aws_region)\n",
    "print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d665e-5721-424c-8dc0-6bf6f9a249ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c214d0f-d6de-4cc5-a8d6-77c1ae4c1016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Select a JumpStart pre-trained model from the dropdown below"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ea755711694f6d884da523db670707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='JumpStart Image Classification Models:', index=6, layout=Layout(width='max-content'), op…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import IPython\n",
    "from ipywidgets import Dropdown\n",
    "\n",
    "# JumpStartの model_manifest ファイルをダウンロードします\n",
    "boto3.client(\"s3\").download_file(\n",
    "    f\"jumpstart-cache-prod-{aws_region}\", \"models_manifest.json\", \"models_manifest.json\"\n",
    ")\n",
    "with open(\"models_manifest.json\", \"rb\") as json_file:\n",
    "    model_list = json.load(json_file)\n",
    "\n",
    "# manifestファイルから全ての画像分類モデルを選択します\n",
    "ic_models_all_versions, ic_models = [\n",
    "    model[\"model_id\"] for model in model_list if \"-ic-\" in model[\"model_id\"]\n",
    "], []\n",
    "[ic_models.append(model) for model in ic_models_all_versions if model not in ic_models]\n",
    "\n",
    "# ユーザ選択のため、model-idsのドロップダウンリストを表示します\n",
    "dropdown = Dropdown(\n",
    "    options=ic_models,\n",
    "    value=model_id,\n",
    "    description=\"JumpStart Image Classification Models:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(IPython.display.Markdown(\"## Select a JumpStart pre-trained model from the dropdown below\"))\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c898ca06-d3d5-4207-af7f-ed094ecc6e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5eb0070d-8099-4ac3-bf27-3300f9a11148",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------JumpStart info----------\n",
      "Endpoint name: jumpstart-example-pytorch-ic-densenet16-2024-03-18-03-43-30-258\n",
      "Container image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.10.0-gpu-py38\n",
      "Source uri: s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/pytorch/transfer_learning/ic/prepack/v1.1.0/sourcedir.tar.gz\n",
      "Base model uri: s3://jumpstart-cache-prod-us-east-1/pytorch-training/v2.0.0/train-pytorch-ic-densenet169.tar.gz\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "# model_version=\"*\" は最新のモデルバージョンを取得します\n",
    "model_id, model_version = dropdown.value, \"*\"\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "training_instance_type = \"ml.g4dn.2xlarge\"\n",
    "\n",
    "# training 用Dockerコンテナを取得します\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"training\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "# training 用スクリプトを取得します\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, \n",
    "    model_version=model_version, \n",
    "    script_scope=\"training\"\n",
    ")\n",
    "# ベースモデルのuriを取得します\n",
    "base_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, \n",
    "    model_version=model_version, \n",
    "    model_scope=\"training\"\n",
    ")\n",
    "print('----------JumpStart info----------')\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "print('Container image uri: ' + train_image_uri)\n",
    "print('Source uri: ' + train_source_uri)\n",
    "print('Base model uri: ' + base_model_uri)\n",
    "print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "947d8548-d2ec-4b8d-81fb-eba2c8299870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 有効なバケット内のサンプル教師画像\n",
    "training_data_bucket = f\"jumpstart-cache-prod-{aws_region}\"\n",
    "training_data_prefix = \"training-datasets/tf_flowers/\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{training_data_bucket}/{training_data_prefix}\"\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"jumpstart-example-ic-training\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84784f54-35df-4e4b-b8df-1bab85c2d815",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_only_top_layer': 'True', 'epochs': '5', 'learning_rate': '0.001', 'batch_size': '4', 'reinitialize_top_layer': 'Auto'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# モデルのファインチューニングのためのデフォルトのハイパーパラメータを取得します\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# [オプション] デフォルトのハイパーパラメータを独自の値で上書きします\n",
    "hyperparameters[\"epochs\"] = \"5\"\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8d05f41-f783-462d-88f6-52d59a8d3542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# チューニングとモデルの選択にAMT (Automatic Model Tuning)を利用するかどうか \n",
    "use_amt = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d9443-61b2-45ea-8a76-0c71ac275a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "62a13d7e-7f25-4b1e-89b6-97aebd83bee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download sourcedir from S3\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists('source'): # ディレクトリが存在するか確認\n",
    "    os.makedirs('source')\n",
    "sourcedir_path = train_source_uri.replace('s3://' + training_data_bucket + '/','')\n",
    "boto3.client(\"s3\").download_file(training_data_bucket, sourcedir_path, './source/sourcedir.tar.gz')\n",
    "shutil.unpack_archive('./source/sourcedir.tar.gz', extract_dir='./source/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7df576b1-e8eb-4d20-8bdf-3b6be5cf75b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"For more information, please refer to PyTorch and SageMaker documentation.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33mhttps://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\u001b[39;49;00m\n",
      "\u001b[33mhttps://pytorch.org/tutorials/beginner/saving_loading_models.html\u001b[39;49;00m\n",
      "\u001b[33mhttps://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html\u001b[39;49;00m\n",
      "\u001b[33mhttps://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#load-a-model\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcopy\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Any\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dict\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m List\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Tuple\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mconstants\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m constants\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_jumpstart_prepack_script_utilities\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mprepack_inference\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m copy_inference_code\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m transforms\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "root = logging.getLogger()\u001b[37m\u001b[39;49;00m\n",
      "root.setLevel(logging.INFO)\u001b[37m\u001b[39;49;00m\n",
      "handler = logging.StreamHandler(sys.stdout)\u001b[37m\u001b[39;49;00m\n",
      "root.addHandler(handler)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda:0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu:0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)))\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--pretrained-model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_MODEL\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=constants.DEFAULT_EPOCHS)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=constants.DEFAULT_BATCH_SIZE)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=constants.DEFAULT_LEARNING_RATE)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--validation_split\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=constants.DEFAULT_VALIDATION_SPLIT)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--reinitialize_top_layer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=constants.DEFAULT_REINITIALIZE_TOP_LAYER)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_only_top_layer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=constants.DEFAULT_TRAIN_ONLY_TOP_LAYER)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_prepare_dataloader\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "    data_dir: \u001b[36mstr\u001b[39;49;00m, batch_size: \u001b[36mint\u001b[39;49;00m, validation_split: \u001b[36mfloat\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      ") -> Tuple[Dict[\u001b[36mstr\u001b[39;49;00m, torch.utils.data.dataloader.DataLoader], Dict[\u001b[36mstr\u001b[39;49;00m, \u001b[36mint\u001b[39;49;00m], Dict[Any, \u001b[36mint\u001b[39;49;00m]]:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Read the data directory and return train/val data loaders and data size and class names to index mapping.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Data augmentation and normalization for training\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Just normalization for validation\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    data_transforms = {\u001b[37m\u001b[39;49;00m\n",
      "        constants.TRAIN: transforms.Compose(\u001b[37m\u001b[39;49;00m\n",
      "            [\u001b[37m\u001b[39;49;00m\n",
      "                transforms.RandomResizedCrop(constants.RANDOM_RESIZED_CROP),\u001b[37m\u001b[39;49;00m\n",
      "                transforms.RandomHorizontalFlip(),\u001b[37m\u001b[39;49;00m\n",
      "                transforms.ToTensor(),\u001b[37m\u001b[39;49;00m\n",
      "                transforms.Normalize(constants.NORMALIZE_MEAN, constants.NORMALIZE_STD),\u001b[37m\u001b[39;49;00m\n",
      "            ]\u001b[37m\u001b[39;49;00m\n",
      "        ),\u001b[37m\u001b[39;49;00m\n",
      "        constants.VAL: transforms.Compose(\u001b[37m\u001b[39;49;00m\n",
      "            [\u001b[37m\u001b[39;49;00m\n",
      "                transforms.Resize(constants.RANDOM_RESIZED_CROP),\u001b[37m\u001b[39;49;00m\n",
      "                transforms.CenterCrop(constants.RANDOM_RESIZED_CROP),\u001b[37m\u001b[39;49;00m\n",
      "                transforms.ToTensor(),\u001b[37m\u001b[39;49;00m\n",
      "                transforms.Normalize(constants.NORMALIZE_MEAN, constants.NORMALIZE_STD),\u001b[37m\u001b[39;49;00m\n",
      "            ]\u001b[37m\u001b[39;49;00m\n",
      "        ),\u001b[37m\u001b[39;49;00m\n",
      "    }\u001b[37m\u001b[39;49;00m\n",
      "    full_dataset = datasets.ImageFolder(os.path.join(data_dir))\u001b[37m\u001b[39;49;00m\n",
      "    class_to_idx = full_dataset.class_to_idx\u001b[37m\u001b[39;49;00m\n",
      "    val_size = \u001b[36mint\u001b[39;49;00m(validation_split * \u001b[36mlen\u001b[39;49;00m(full_dataset))\u001b[37m\u001b[39;49;00m\n",
      "    train_size = \u001b[36mlen\u001b[39;49;00m(full_dataset) - val_size\u001b[37m\u001b[39;49;00m\n",
      "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    val_dataset.dataset.transform = data_transforms[constants.VAL]\u001b[37m\u001b[39;49;00m\n",
      "    train_dataset.dataset.transform = data_transforms[constants.TRAIN]\u001b[37m\u001b[39;49;00m\n",
      "    val_dataset.dataset.transforms = data_transforms[constants.VAL]\u001b[37m\u001b[39;49;00m\n",
      "    train_dataset.dataset.transforms = data_transforms[constants.TRAIN]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    dataloaders = {\u001b[37m\u001b[39;49;00m\n",
      "        constants.TRAIN: torch.utils.data.DataLoader(\u001b[37m\u001b[39;49;00m\n",
      "            train_dataset, batch_size=batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m, num_workers=constants.NUM_WORKERS\u001b[37m\u001b[39;49;00m\n",
      "        ),\u001b[37m\u001b[39;49;00m\n",
      "        constants.VAL: torch.utils.data.DataLoader(\u001b[37m\u001b[39;49;00m\n",
      "            val_dataset, batch_size=batch_size, num_workers=constants.NUM_WORKERS\u001b[37m\u001b[39;49;00m\n",
      "        ),\u001b[37m\u001b[39;49;00m\n",
      "    }\u001b[37m\u001b[39;49;00m\n",
      "    dataset_sizes = {constants.TRAIN: \u001b[36mlen\u001b[39;49;00m(train_dataset), constants.VAL: \u001b[36mlen\u001b[39;49;00m(val_dataset)}\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m dataloaders, dataset_sizes, class_to_idx\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_model\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "    dataloaders: Dict[\u001b[36mstr\u001b[39;49;00m, Any],\u001b[37m\u001b[39;49;00m\n",
      "    dataset_sizes: Dict[\u001b[36mstr\u001b[39;49;00m, \u001b[36mint\u001b[39;49;00m],\u001b[37m\u001b[39;49;00m\n",
      "    model,\u001b[37m\u001b[39;49;00m\n",
      "    criterion: torch.nn.modules.loss.CrossEntropyLoss,\u001b[37m\u001b[39;49;00m\n",
      "    optimizer,\u001b[37m\u001b[39;49;00m\n",
      "    scheduler: torch.optim.lr_scheduler,\u001b[37m\u001b[39;49;00m\n",
      "    num_epochs: \u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      ") -> collections.OrderedDict:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Run training and record time.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    since = time.time()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    best_model_wts = copy.deepcopy(model.state_dict())\u001b[37m\u001b[39;49;00m\n",
      "    best_acc = \u001b[34m0.0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(num_epochs):\u001b[37m\u001b[39;49;00m\n",
      "        logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mEpoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epoch, num_epochs - \u001b[34m1\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Each epoch has a training and validation phase\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m phase \u001b[35min\u001b[39;49;00m [constants.TRAIN, constants.VAL]:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m phase == constants.TRAIN:\u001b[37m\u001b[39;49;00m\n",
      "                model.train()  \u001b[37m# Set model to training mode\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "                model.eval()  \u001b[37m# Set model to evaluate mode\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            running_loss = \u001b[34m0.0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            running_corrects = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m# Iterate over data.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mfor\u001b[39;49;00m inputs, labels \u001b[35min\u001b[39;49;00m dataloaders[phase]:\u001b[37m\u001b[39;49;00m\n",
      "                inputs = inputs.to(device)\u001b[37m\u001b[39;49;00m\n",
      "                labels = labels.to(device)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[37m# zero the parameter gradients\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                optimizer.zero_grad()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[37m# forward\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[37m# track history if only in train\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[34mwith\u001b[39;49;00m torch.set_grad_enabled(phase == constants.TRAIN):\u001b[37m\u001b[39;49;00m\n",
      "                    outputs = model(inputs)\u001b[37m\u001b[39;49;00m\n",
      "                    _, preds = torch.max(outputs, \u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "                    loss = criterion(outputs, labels)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                    \u001b[37m# backward + optimize only if in training phase\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                    \u001b[34mif\u001b[39;49;00m phase == constants.TRAIN:\u001b[37m\u001b[39;49;00m\n",
      "                        loss.backward()\u001b[37m\u001b[39;49;00m\n",
      "                        optimizer.step()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[37m# statistics\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                running_loss += loss.item() * inputs.size(\u001b[34m0\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "                running_corrects += torch.sum(preds == labels.data)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m phase == constants.TRAIN:\u001b[37m\u001b[39;49;00m\n",
      "                scheduler.step()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            epoch_loss = running_loss / dataset_sizes[phase]\u001b[37m\u001b[39;49;00m\n",
      "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m Loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m Acc: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(phase, epoch_loss, phase, epoch_acc))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m# deep copy the model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m phase == constants.VAL \u001b[35mand\u001b[39;49;00m epoch_acc > best_acc:\u001b[37m\u001b[39;49;00m\n",
      "                best_acc = epoch_acc\u001b[37m\u001b[39;49;00m\n",
      "                best_model_wts = copy.deepcopy(model.state_dict())\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    time_elapsed = time.time() - since\u001b[37m\u001b[39;49;00m\n",
      "    logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining complete in \u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33mm \u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33ms\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(time_elapsed // \u001b[34m60\u001b[39;49;00m, time_elapsed % \u001b[34m60\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mBest val Acc: \u001b[39;49;00m\u001b[33m{:4f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(best_acc))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m best_model_wts\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mchange_pred_layer_size\u001b[39;49;00m(model: Any, model_id: \u001b[36mstr\u001b[39;49;00m, _size: \u001b[36mint\u001b[39;49;00m) -> \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Mutate the model to substitute its last layer.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36many\u001b[39;49;00m(model \u001b[35min\u001b[39;49;00m model_id \u001b[34mfor\u001b[39;49;00m model \u001b[35min\u001b[39;49;00m (\u001b[33m\"\u001b[39;49;00m\u001b[33mresnet\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mresnext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mshufflenet\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mgooglenet\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)):\u001b[37m\u001b[39;49;00m\n",
      "        num_features = model.fc.in_features\u001b[37m\u001b[39;49;00m\n",
      "        model.fc = torch.nn.Linear(num_features, _size)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melif\u001b[39;49;00m \u001b[36many\u001b[39;49;00m(model \u001b[35min\u001b[39;49;00m model_id \u001b[34mfor\u001b[39;49;00m model \u001b[35min\u001b[39;49;00m (\u001b[33m\"\u001b[39;49;00m\u001b[33mmobilenet\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33malexnet\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mvgg\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)):\u001b[37m\u001b[39;49;00m\n",
      "        pos_linear_layer = \u001b[36mlen\u001b[39;49;00m(model.classifier) - \u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        num_features = model.classifier[pos_linear_layer].in_features\u001b[37m\u001b[39;49;00m\n",
      "        model.classifier._modules[\u001b[36mstr\u001b[39;49;00m(pos_linear_layer)] = torch.nn.Linear(num_features, _size)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdensenet\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m model_id:\u001b[37m\u001b[39;49;00m\n",
      "        num_features = model.classifier.in_features\u001b[37m\u001b[39;49;00m\n",
      "        model.classifier = torch.nn.Linear(num_features, _size)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33msqueezenet\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m model_id:\u001b[37m\u001b[39;49;00m\n",
      "        model.classifier._modules[\u001b[33m\"\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = torch.nn.Conv2d(\u001b[34m512\u001b[39;49;00m, _size, kernel_size=(\u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mNotImplementedError\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_label_info\u001b[39;49;00m(class_to_idx: Dict[Any, \u001b[36mint\u001b[39;49;00m], model_dir: \u001b[36mstr\u001b[39;49;00m) -> \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Save labels with the model.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    idx_label_map: Dict[\u001b[36mint\u001b[39;49;00m, Any] = {idx: label \u001b[34mfor\u001b[39;49;00m label, idx \u001b[35min\u001b[39;49;00m class_to_idx.items()}\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    labels = [idx_label_map[idx] \u001b[34mfor\u001b[39;49;00m idx \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(idx_label_map))]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, constants.LABELS_INFO), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "        f.write(json.dumps({constants.LABELS: labels}))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mis_input_model_finetuned\u001b[39;49;00m(input_model_untarred_path: \u001b[36mstr\u001b[39;49;00m) -> \u001b[36mbool\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Read the model_info file from file system to check if the input model was already finetuned.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    If the model_info file does not exists or if it does not contain information about finetuning, we assume that\u001b[39;49;00m\n",
      "\u001b[33m    the model is not previously finetuned.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    input_model_info_file_path = os.path.join(input_model_untarred_path, constants.MODEL_INFO_FILE_NAME)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mtry\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(input_model_info_file_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "            model_info = json.load(f)\u001b[37m\u001b[39;49;00m\n",
      "            is_finetuned = model_info[constants.FINETUNED]\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34massert\u001b[39;49;00m \u001b[36mtype\u001b[39;49;00m(is_finetuned) == \u001b[36mbool\u001b[39;49;00m, (\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info file corrupted \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconstants.FINETUNED\u001b[33m}\u001b[39;49;00m\u001b[33m parameter must be \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33meither True or False, got \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mis_finetuned\u001b[33m}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mreturn\u001b[39;49;00m is_finetuned\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00minput_model_info_file_path\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m file could not be found.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mKeyError\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconstants.FINETUNED\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m not in model info file. Assuming model was not fine-tuned.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\u001b[37m\u001b[39;49;00m\n",
      "        logging.error(\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCould not read or parse model_info file, exception: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00me\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m. To continue finetuning on \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mpreviously finetuned model, please create a json file \u001b[39;49;00m\u001b[33m{\u001b[39;49;00minput_model_info_file_path\u001b[33m}\u001b[39;49;00m\u001b[33m with\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconstants.FINETUNED\u001b[33m}\u001b[39;49;00m\u001b[33m parameter set to True. To start finetuning from scratch,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mplease create a json file \u001b[39;49;00m\u001b[33m{\u001b[39;49;00minput_model_info_file_path\u001b[33m}\u001b[39;49;00m\u001b[33m with\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconstants.FINETUNED\u001b[33m}\u001b[39;49;00m\u001b[33m parameter set to False.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mraise\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model_info\u001b[39;49;00m(input_model_untarred_path: \u001b[36mstr\u001b[39;49;00m, model_dir: \u001b[36mstr\u001b[39;49;00m) -> \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Save model info to the output directory along with finetuned parameter set to True.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    Read the existing model_info file in input_model directory if exists, set finetuned parameter to True and saves\u001b[39;49;00m\n",
      "\u001b[33m    it in the output model directory.\u001b[39;49;00m\n",
      "\u001b[33m    Args:\u001b[39;49;00m\n",
      "\u001b[33m        input_model_untarred_path: Input model is untarred into this directory.\u001b[39;49;00m\n",
      "\u001b[33m        model_dir: Output model directory.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    input_model_info_file_path = os.path.join(input_model_untarred_path, constants.MODEL_INFO_FILE_NAME)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mtry\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(input_model_info_file_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "            model_info: Dict[\u001b[36mstr\u001b[39;49;00m, Any] = json.load(f)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mInfo file not found at \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00minput_model_info_file_path\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        model_info: Dict[\u001b[36mstr\u001b[39;49;00m, Any] = {}\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\u001b[37m\u001b[39;49;00m\n",
      "        logging.error(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mError parsing model_info file: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00me\u001b[33m}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mraise\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_info[constants.FINETUNED] = \u001b[34mTrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    output_model_info_file_path = os.path.join(model_dir, constants.MODEL_INFO_FILE_NAME)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(output_model_info_file_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "        f.write(json.dumps(model_info))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_top_layer_param_names\u001b[39;49;00m(model: Any) -> List[\u001b[36mstr\u001b[39;49;00m]:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Return the names of parameters of the top layer.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mhasattr\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "        model, \u001b[33m\"\u001b[39;49;00m\u001b[33mfc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    ):  \u001b[37m# Top layer is fc. It happens for models based on \"resnet\", \"resnext\", \"shufflenet\", \"googlenet\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m [\u001b[33m\"\u001b[39;49;00m\u001b[33mfc.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + param_name \u001b[34mfor\u001b[39;49;00m param_name, _ \u001b[35min\u001b[39;49;00m model.fc.named_parameters()]\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m [\u001b[33m\"\u001b[39;49;00m\u001b[33mclassifier.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + param_name \u001b[34mfor\u001b[39;49;00m param_name, _ \u001b[35min\u001b[39;49;00m model.classifier.named_parameters()]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mset_requires_grad\u001b[39;49;00m(model: Any, train_only_top_layer: \u001b[36mbool\u001b[39;49;00m) -> \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Set the require_grad attribute for each parameter based on train_only_top_layer.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    Parameters in the last layer are always trainable (requires_grad = True).\u001b[39;49;00m\n",
      "\u001b[33m    For rest of the parameters, requires_grad = not train_only_top_layer.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    top_layer_param_names = get_top_layer_param_names(model)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m param_name, param \u001b[35min\u001b[39;49;00m model.named_parameters():\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m param_name \u001b[35mnot\u001b[39;49;00m \u001b[35min\u001b[39;49;00m top_layer_param_names:\u001b[37m\u001b[39;49;00m\n",
      "            param.requires_grad = \u001b[35mnot\u001b[39;49;00m train_only_top_layer\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            param.requires_grad = \u001b[34mTrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mrun_with_args\u001b[39;49;00m(args):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Run training.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    dataloaders, dataset_sizes, class_to_idx = _prepare_dataloader(\u001b[37m\u001b[39;49;00m\n",
      "        args.train, batch_size=args.batch_size, validation_split=args.validation_split\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mdataset sizes: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdataset_sizes\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mprediction class indices mapping to input training data labels: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mclass_to_idx\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    num_labels = \u001b[36mlen\u001b[39;49;00m(class_to_idx.keys())\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    input_model_path = \u001b[36mnext\u001b[39;49;00m(pathlib.Path(args.pretrained_model).glob(\u001b[33m\"\u001b[39;49;00m\u001b[33m*.tar.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(constants.INPUT_MODEL_UNTARRED_PATH):\u001b[37m\u001b[39;49;00m\n",
      "        os.mkdir(constants.INPUT_MODEL_UNTARRED_PATH)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# extract model files\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m tarfile.open(input_model_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m saved_model_tar:\u001b[37m\u001b[39;49;00m\n",
      "        saved_model_tar.extractall(constants.INPUT_MODEL_UNTARRED_PATH)\u001b[37m\u001b[39;49;00m\n",
      "    input_model_file_name = \u001b[36mnext\u001b[39;49;00m(pathlib.Path(constants.INPUT_MODEL_UNTARRED_PATH).glob(\u001b[33m\"\u001b[39;49;00m\u001b[33m*.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)).stem + \u001b[33m\"\u001b[39;49;00m\u001b[33m.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    input_model_file_path = os.path.join(constants.INPUT_MODEL_UNTARRED_PATH, input_model_file_name)\u001b[37m\u001b[39;49;00m\n",
      "    model = torch.load(input_model_file_path)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    is_finetuned = is_input_model_finetuned(constants.INPUT_MODEL_UNTARRED_PATH)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# If the network is not already finetuned, then following determines whether we replace the top layer or not\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# auto -> replace, true -> replace, false -> do not replace\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# If network is finetuned, then the following determines whether we replace the top layer or not\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# auto -> do not replace, true -> replace, false -> do not replace.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m (\u001b[35mnot\u001b[39;49;00m is_finetuned \u001b[35mand\u001b[39;49;00m args.reinitialize_top_layer != constants.FALSE) \u001b[35mor\u001b[39;49;00m (\u001b[37m\u001b[39;49;00m\n",
      "        is_finetuned \u001b[35mand\u001b[39;49;00m args.reinitialize_top_layer == constants.TRUE\u001b[37m\u001b[39;49;00m\n",
      "    ):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Remove the pre-existing top layer from pre-trained network and insert a new layer corresonding to the training\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# data with number of outputs same as the number of labels in the training data.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        change_pred_layer_size(model, input_model_file_path, num_labels)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    set_requires_grad(model, args.train_only_top_layer == constants.TRUE)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m torch.cuda.is_available():\u001b[37m\u001b[39;49;00m\n",
      "        model.to(device)\u001b[37m\u001b[39;49;00m\n",
      "    model.eval()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    criterion = torch.nn.CrossEntropyLoss()\u001b[37m\u001b[39;49;00m\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\u001b[37m\u001b[39;49;00m\n",
      "    exp_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=constants.EXPONENTIAL_LR_GAMMA)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    best_model_wts = train_model(\u001b[37m\u001b[39;49;00m\n",
      "        dataloaders, dataset_sizes, model, criterion, optimizer, exp_lr_scheduler, num_epochs=args.epochs\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    org_model = torch.load(input_model_file_path)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m is_finetuned:\u001b[37m\u001b[39;49;00m\n",
      "        change_pred_layer_size(org_model, input_model_file_path, num_labels)\u001b[37m\u001b[39;49;00m\n",
      "    org_model.load_state_dict(best_model_wts)\u001b[37m\u001b[39;49;00m\n",
      "    torch.save(org_model, os.path.join(args.model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# saving list of labels to model_dir\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    save_label_info(class_to_idx, model_dir=args.model_dir)\u001b[37m\u001b[39;49;00m\n",
      "    save_model_info(input_model_untarred_path=constants.INPUT_MODEL_UNTARRED_PATH, model_dir=args.model_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    args, unknown = _parse_args()\u001b[37m\u001b[39;49;00m\n",
      "    run_with_args(args)\u001b[37m\u001b[39;49;00m\n",
      "    copy_inference_code(dst_path=args.model_dir)\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./source/transfer_learning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157f604-6ce6-4c14-ad1d-54a76dd9f31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be04113e-4f42-4d66-97dc-01912a8c6aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: jumpstart-example-pytorch-ic-densenet16-2024-03-18-03-43-38-648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-18 03:43:38 Starting - Starting the training job...\n",
      "2024-03-18 03:43:58 Starting - Preparing the instances for training...\n",
      "2024-03-18 03:44:31 Downloading - Downloading input data...\n",
      "2024-03-18 03:44:56 Downloading - Downloading the training image...............\n",
      "2024-03-18 03:47:40 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-03-18 03:48:06,074 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-03-18 03:48:06,106 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-03-18 03:48:06,110 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-03-18 03:48:06,256 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_prepack_script_utilities/sagemaker_jumpstart_prepack_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker-jumpstart-prepack-script-utilities\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-jumpstart-prepack-script-utilities-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-03-18 03:48:07,754 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": \"4\",\n",
      "        \"epochs\": \"5\",\n",
      "        \"learning_rate\": \"0.001\",\n",
      "        \"reinitialize_top_layer\": \"Auto\",\n",
      "        \"train_only_top_layer\": \"True\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"jumpstart-example-pytorch-ic-densenet16-2024-03-18-03-43-38-648\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/pytorch/transfer_learning/ic/prepack/v1.1.0/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":\"4\",\"epochs\":\"5\",\"learning_rate\":\"0.001\",\"reinitialize_top_layer\":\"Auto\",\"train_only_top_layer\":\"True\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/pytorch/transfer_learning/ic/prepack/v1.1.0/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":\"4\",\"epochs\":\"5\",\"learning_rate\":\"0.001\",\"reinitialize_top_layer\":\"Auto\",\"train_only_top_layer\":\"True\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"jumpstart-example-pytorch-ic-densenet16-2024-03-18-03-43-38-648\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/pytorch/transfer_learning/ic/prepack/v1.1.0/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"4\",\"--epochs\",\"5\",\"--learning_rate\",\"0.001\",\"--reinitialize_top_layer\",\"Auto\",\"--train_only_top_layer\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_REINITIALIZE_TOP_LAYER=Auto\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_ONLY_TOP_LAYER=True\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 transfer_learning.py --batch_size 4 --epochs 5 --learning_rate 0.001 --reinitialize_top_layer Auto --train_only_top_layer True\u001b[0m\n",
      "\u001b[34mdataset sizes: {'train': 2936, 'val': 734}\u001b[0m\n",
      "\u001b[34mprediction class indices mapping to input training data labels: {'daisy': 0, 'dandelion': 1, 'roses': 2, 'sunflowers': 3, 'tulips': 4}\u001b[0m\n",
      "\u001b[34m'_input_model_extracted/__models_info__.json' file could not be found.\u001b[0m\n",
      "\u001b[34mEpoch 0/4\u001b[0m\n",
      "\u001b[34m[2024-03-18 03:48:12.383 algo-1:33 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-03-18 03:48:12.416 algo-1:33 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-03-18 03:48:12.417 algo-1:33 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-03-18 03:48:12.417 algo-1:33 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-03-18 03:48:12.417 algo-1:33 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-03-18 03:48:12.417 algo-1:33 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-03-18 03:48:12.675 algo-1:33 INFO hook.py:591] name:classifier.weight count_params:8320\u001b[0m\n",
      "\u001b[34m[2024-03-18 03:48:12.675 algo-1:33 INFO hook.py:591] name:classifier.bias count_params:5\u001b[0m\n",
      "\u001b[34m[2024-03-18 03:48:12.675 algo-1:33 INFO hook.py:593] Total Trainable Params: 8325\u001b[0m\n",
      "\u001b[34m[2024-03-18 03:48:12.675 algo-1:33 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2024-03-18 03:48:12.677 algo-1:33 INFO hook.py:488] Hook is writing from the hook with pid: 33\u001b[0m\n",
      "\u001b[34mtrain Loss: 0.9412 train Acc: 0.6434\u001b[0m\n",
      "\u001b[34mval Loss: 0.4116 val Acc: 0.8529\u001b[0m\n",
      "\u001b[34mEpoch 1/4\u001b[0m\n",
      "\u001b[34mtrain Loss: 0.6523 train Acc: 0.7687\u001b[0m\n",
      "\u001b[34mval Loss: 0.4109 val Acc: 0.8583\u001b[0m\n",
      "\u001b[34mEpoch 2/4\u001b[0m\n",
      "\u001b[34mtrain Loss: 0.6355 train Acc: 0.7663\u001b[0m\n",
      "\u001b[34mval Loss: 0.3866 val Acc: 0.8624\u001b[0m\n",
      "\u001b[34mEpoch 3/4\u001b[0m\n",
      "\u001b[34mtrain Loss: 0.6698 train Acc: 0.7698\u001b[0m\n",
      "\u001b[34mval Loss: 0.4075 val Acc: 0.8392\u001b[0m\n",
      "\u001b[34mEpoch 4/4\u001b[0m\n",
      "\u001b[34mtrain Loss: 0.6312 train Acc: 0.7820\u001b[0m\n",
      "\u001b[34mval Loss: 0.4109 val Acc: 0.8515\u001b[0m\n",
      "\u001b[34mTraining complete in 4m 16s\u001b[0m\n",
      "\u001b[34mBest val Acc: 0.862398\u001b[0m\n",
      "\u001b[34mInfo file not found at '_input_model_extracted/__models_info__.json'.\u001b[0m\n",
      "\u001b[34m2024-03-18 03:52:29,628 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-03-18 03:52:42 Uploading - Uploading generated training model\n",
      "2024-03-18 03:52:42 Completed - Training job completed\n",
      "Training seconds: 491\n",
      "Billable seconds: 491\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "training_job_name = name_from_base(f\"jumpstart-example-{model_id}-transfer-learning\")\n",
    "\n",
    "# SageMakerのEstimatorインスタンスを作成します\n",
    "ic_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=base_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    ")\n",
    "\n",
    "# トレーニングデータのS3パスを渡して、SageMaker 学習ジョブを開始します\n",
    "ic_estimator.fit({\"training\": training_dataset_s3_path}, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a67e3e3-3cf7-46a2-93bf-14eaf29a851a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eadce022-dae1-481b-a620-4cada6442cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-east-1-222426255091/jumpstart-example-ic-training/output/jumpstart-example-pytorch-ic-densenet16-2024-03-18-03-43-38-648/output/model.tar.gz), script artifact (s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/pytorch/inference/ic/v2.0.0/sourcedir.tar.gz), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-east-1-222426255091/sagemaker-jumpstart-2024-03-18-04-50-38-518/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: sagemaker-jumpstart-2024-03-18-04-50-38-518\n",
      "INFO:sagemaker:Creating endpoint-config with name jumpstart-example-FT-pytorch-ic-densene-2024-03-18-04-50-38-518\n",
      "INFO:sagemaker:Creating endpoint with name jumpstart-example-FT-pytorch-ic-densene-2024-03-18-04-50-38-518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "# 推論用 Docker コンテナの uri を取得します\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "# 推論用のスクリプト uri を取得します\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-FT-{model_id}-\")\n",
    "\n",
    "# SageMakerエンドポイントをデプロイするため前のステップのestimatorを使用します\n",
    "finetuned_predictor = (hp_tuner if use_amt else ic_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327cf4a-215a-4a6e-b0f1-93abb8981466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "245aff37-514d-4450-b5b0-c8a13cf0d464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_bucket = f\"jumpstart-cache-prod-{aws_region}\"\n",
    "key_prefix = \"training-datasets/tf_flowers\"\n",
    "\n",
    "\n",
    "def download_from_s3(images):\n",
    "    for filename, image_key in images.items():\n",
    "        boto3.client(\"s3\").download_file(s3_bucket, f\"{key_prefix}/{image_key}\", filename)\n",
    "\n",
    "\n",
    "flower_images = {\n",
    "    \"img1.jpg\": \"roses/10503217854_e66a804309.jpg\",\n",
    "    \"img2.jpg\": \"sunflowers/1008566138_6927679c8a.jpg\",\n",
    "}\n",
    "download_from_s3(flower_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd22a04-dc7c-4656-af40-f604c27a4b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cabfe78e-3911-4a94-b655-4c68482e5418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=img1.jpg alt=img1.jpg align=\"left\" style=\"width: 250px;\"/><figcaption>Predicted Label: roses</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=img2.jpg alt=img2.jpg align=\"left\" style=\"width: 250px;\"/><figcaption>Predicted Label: sunflowers</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "for image_filename in flower_images.keys():\n",
    "    with open(image_filename, \"rb\") as file:\n",
    "        img = file.read()\n",
    "    query_response = finetuned_predictor.predict(\n",
    "        img, {\"ContentType\": \"application/x-image\", \"Accept\": \"application/json;verbose\"}\n",
    "    )\n",
    "    model_predictions = json.loads(query_response)\n",
    "    predicted_label = model_predictions[\"predicted_label\"]\n",
    "    display(\n",
    "        HTML(\n",
    "            f'<img src={image_filename} alt={image_filename} align=\"left\" style=\"width: 250px;\"/>'\n",
    "            f\"<figcaption>Predicted Label: {predicted_label}</figcaption>\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2b79995-e923-4866-8386-8a5869336ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: sagemaker-jumpstart-2024-03-18-04-50-38-518\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: jumpstart-example-FT-pytorch-ic-densene-2024-03-18-04-50-38-518\n",
      "INFO:sagemaker:Deleting endpoint with name: jumpstart-example-FT-pytorch-ic-densene-2024-03-18-04-50-38-518\n"
     ]
    }
   ],
   "source": [
    "# SageMakerエンドポイントとアタッチされたリソースを削除します\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37c8eb-a720-409c-b10d-b7a94cfbb35d",
   "metadata": {},
   "source": [
    "## Incremental learinig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8e092d5-c422-4330-822e-aac1cc2739d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-222426255091/jumpstart-example-ic-training/output/jumpstart-example-pytorch-ic-densenet16-2024-03-18-03-43-38-648/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# 学習ジョブ名とアーティファクトのアウトプット場所をもとに、前のステップで学習したモデルを特定します\n",
    "\n",
    "last_training_job_name = ic_estimator._current_job_name\n",
    "\n",
    "last_trained_model_path = f\"{s3_output_location}/{last_training_job_name}/output/model.tar.gz\"\n",
    "\n",
    "print(last_trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "218312f4-1447-49bf-9850-6b48626d98f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: jumpstart-example-pytorch-ic-densenet16-2024-03-18-05-34-18-996\n"
     ]
    },
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g4dn.2xlarge for training job usage' is 1 Instances, with current utilization of 1 Instances and a request delta of 1 Instances. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 21\u001b[0m\n\u001b[1;32m      5\u001b[0m incremental_training_job_name \u001b[38;5;241m=\u001b[39m name_from_base(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjumpstart-example-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-incremental-training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m incremental_train_estimator \u001b[38;5;241m=\u001b[39m Estimator(\n\u001b[1;32m      8\u001b[0m     role\u001b[38;5;241m=\u001b[39maws_role,\n\u001b[1;32m      9\u001b[0m     image_uri\u001b[38;5;241m=\u001b[39mtrain_image_uri,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     base_job_name\u001b[38;5;241m=\u001b[39mincremental_training_job_name,\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[43mincremental_train_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_dataset_s3_path\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:1338\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_for_training(job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[1;32m   1337\u001b[0m experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[0;32m-> 1338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m \u001b[43m_TrainingJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:2437\u001b[0m, in \u001b[0;36m_TrainingJob.start_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a new Amazon SageMaker training job from the estimator.\u001b[39;00m\n\u001b[1;32m   2413\u001b[0m \n\u001b[1;32m   2414\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2433\u001b[0m \u001b[38;5;124;03m    all information about the started training job.\u001b[39;00m\n\u001b[1;32m   2434\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m train_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_train_args(estimator, inputs, experiment_config)\n\u001b[0;32m-> 2437\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(estimator\u001b[38;5;241m.\u001b[39msagemaker_session, estimator\u001b[38;5;241m.\u001b[39m_current_job_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:982\u001b[0m, in \u001b[0;36mSession.train\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, training_image_config, infra_check_config, container_entry_point, container_arguments, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy, remote_debug_config)\u001b[0m\n\u001b[1;32m    979\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_training_job(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\n\u001b[0;32m--> 982\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:6427\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   6410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[1;32m   6411\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   6412\u001b[0m     request: typing\u001b[38;5;241m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6415\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   6416\u001b[0m ):\n\u001b[1;32m   6417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   6418\u001b[0m \n\u001b[1;32m   6419\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6425\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   6426\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:980\u001b[0m, in \u001b[0;36mSession.train.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    978\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating training-job with name: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, job_name)\n\u001b[1;32m    979\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m--> 980\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g4dn.2xlarge for training job usage' is 1 Instances, with current utilization of 1 Instances and a request delta of 1 Instances. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota."
     ]
    }
   ],
   "source": [
    "incremental_train_output_prefix = \"jumpstart-example-ic-incremental-training\"\n",
    "\n",
    "incremental_s3_output_location = f\"s3://{output_bucket}/{incremental_train_output_prefix}/output\"\n",
    "\n",
    "incremental_training_job_name = name_from_base(f\"jumpstart-example-{model_id}-incremental-training\")\n",
    "\n",
    "incremental_train_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=last_trained_model_path,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=incremental_s3_output_location,\n",
    "    base_job_name=incremental_training_job_name,\n",
    ")\n",
    "\n",
    "incremental_train_estimator.fit({\"training\": training_dataset_s3_path}, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df80260a-0a52-49db-a51d-6bd279db51f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa24d1c-57f4-4fea-b06c-ad7851648d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb82565-720d-4c17-8884-1058568f338d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613146f4-77da-4d97-af36-8e6f36a7a1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
